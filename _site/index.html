<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
<title>DLF Metadata Assessment Working Group</title>
<meta name="viewport" content="width=device-width">
<meta name="author" content="DLF AIG Metadata Working Group">
<link rel="stylesheet" href="css/main.css">
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:500,600' rel='stylesheet' type='text/css'>
    </head>

    <body>
        <div class="container">
            <div class="overview">
                <span class="toggle">close</span>
                <header>
    <span>
        <img size="100%" class="img-responsive" class="center-block" src="../img/dlf-aig.png"/>
    </span>
</header>

                <ul id="nav">
    
    
    <li  class=" current">
        <a href="#about">DLF AIG Metadata Working Group</a>
        
    </li>
    
    <li  class="parent">
        <a href="#take-part">Take Part</a>
        
            <ul>
                
                
            </ul>
        
    </li>
    
    <li  class="parent">
        <a href="#review">Environmental Scan</a>
        
            <ul>
                
                
                    <li class="parent">
                        <a href="#review-orgs">Organizations &amp; Groups</a>
                        
                            <ul>
                                
                                
                            </ul>
                        
                    </li>
                
                    <li class="parent">
                        <a href="#review-pres">Presentations</a>
                        
                            <ul>
                                
                                
                            </ul>
                        
                    </li>
                
                    <li class="parent">
                        <a href="#review-pubs">Publications</a>
                        
                            <ul>
                                
                                
                            </ul>
                        
                    </li>
                
                    <li class="parent">
                        <a href="#review-tools">Tools</a>
                        
                            <ul>
                                
                                
                            </ul>
                        
                    </li>
                
                    <li class="parent">
                        <a href="#citations">Citations</a>
                        
                            <ul>
                                
                                
                            </ul>
                        
                    </li>
                
            </ul>
        
    </li>
    
    <li  class="parent">
        <a href="#resources">Resources &amp; Contact</a>
        
            <ul>
                
                
            </ul>
        
    </li>
    
</ul>
            </div>

            <div class="content">
                <span class="toggle" id="menu">menu</span>
                

    
        <section id="about" class="h1">
            
                    <h1>DLF AIG Metadata Working Group</h1>
                

            <p>This is the website for the <strong><a href="https://wiki.diglib.org/Assessment:Metadata">Digital Library Federation (DLF) Assessment Interest Group (AIG) Metadata Working Group</a></strong>, also known as the DLF Metadata Assessment Working Group.</p>

<p>The DLF Metadata Assessment Working Group aims to build guidelines, best practices, tools and workflows around the evaluation and assessment of (mostly, descriptive) metadata used by and for digital libraries and repositories. The foci of the work of the 2016 DLF Metadata Assessment Working Group are:</p>

<ol>
  <li>performing an environmental scan on the topic of metadata quality and assessment;</li>
  <li>gathering use cases and definitions for metadata assessment needs and realities;</li>
  <li>creating a preliminary framework and set of recommendations on metadata assessment.</li>
</ol>

        </section>
    

    
        <section id="take-part" class="h1">
            
                    <h1>Take Part</h1>
                

            <p>We want this website and the information contained therein to be a living document. Working Group members will continue to add resources, recommendations, and information to this website as well as to our working documents. If you want to make a comment, suggestion, correction or update, you can:</p>

<ul>
  <li>
    <ul>
      <li><a href="https://github.com/DLFMetadataAssessment/DLFMetadataAssessment.github.io/pulls">submit a pull request to this website,</a></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><a href="https://github.com/DLFMetadataAssessment/DLFMetadataAssessment.github.io/issues">open a GitHub issue on this repository,</a></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><a href="https://drive.google.com/open?id=0B74oOQcTdnHjWk51a283bUVta0E">comment on our working docs in this Google Drive folder,</a></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><a href="https://groups.google.com/forum/#!forum/dlf-aig-metadata-assessment-working-group">send a message to our Google Group,</a></li>
    </ul>
  </li>
  <li>
    <ul>
      <li><a href="https://wiki.diglib.org/Assessment:Metadata">or join one of our calls.</a></li>
    </ul>
  </li>
</ul>

<p>Anyone and everyone is welcomed to take part in the DLF AIG Metadata Working Group. Use the link below to find out more about the group or get involved.</p>

<ul>
  <li>
    <ul>
      <li><a href="https://wiki.diglib.org/Assessment:Metadata">our DLF wiki page</a></li>
    </ul>
  </li>
</ul>

        </section>
    

    
        <section id="review" class="h1">
            
                    <h1>Environmental Scan</h1>
                

            <p>This is our first area of work for the 2016 DLF Metadata Assessment group. We performed a literature, tools, presentations, and organizations review on the topics of metadata assessment and metadata quality - with a focus on (but not limited to) digital repositories descriptive metadata.</p>

        </section>
    

    
        <section id="review-orgs" class="h2">
            
                    <h2>Organizations &amp; Groups</h2>
                

            <p><a href="https://docs.google.com/document/d/1rk6TThrSqpLNk-L0JgR3lk5b_M3M8n5xM2xggKHYVUw/edit#heading=h.ur4v6u3nkorp">Draft and Notes for this Section</a></p>

<h3>Summary</h3>

<p>A wide range of groups are addressing issues related to metadata and issues related to assessment of library services, but relatively few are directly working on the assessment of metadata. Here are some organizations and resources of interest.</p>

<h3>Europeana</h3>

<p><a href="http://pro.europeana.eu/">Europeana</a> is actively working to develop quality standards for metadata. The <a href="http://pro.europeana.eu/page/data-quality-committee">Data Quality Committee</a> is addressing many issues related to metadata, including required elements for ingest of EDM data and meaningful metadata values in the context of use. “This work includes measures for information value of statements (informativeness, degree of multilinguality…) “ (p. 3). Of particular note is the committee’s statement on data quality: “The Committee considers that data quality is always relative to intended use and cannot be analysed or defined in isolation from it, as a theoretical effort” (p. 1).</p>

<p><a href="http://pro.europeana.eu/files/Europeana_Professional/Publications/Metadata%20Quality%20Report.pdf">Europeana’s Report and Recommendations from the Task Force on Metadata Quality</a> is an essential read, outlining broad issues related to metadata quality as well as specific recommendations for the Europeana community. This report defines good metadata quality as “1. Resulting from a series of trusted processes 2. Findable 3. Readable 4. Standardised 5. Meaningful to audiences 6. Clear on re-use 7. Visible” (p. 3). In addition, the report explores hindrances to good metadata quality: lack of foresight for online discovery, treating metadata as an afterthought, lack of funding and resources, describing digitized items with little information, and not understanding the harvesting requirements.</p>

<p>The <a href="http://pro.europeana.eu/files/Europeana_Professional/EuropeanaTech/EuropeanaTech_taskforces/Enrichment_Evaluation/FinalReport_EnrichmentEvaluation_102015.pdf">Task Force on Enrichment and Evaluation’s Final Report</a> provides ten recommendations for successful enrichment strategies. See Valentine Charles and Juliane Stiller’s presentation <a href="https://www.youtube.com/watch?v=U90Ajgjk6ic">Evaluation of Metadata Enrichment Practices in Digital Libraries</a> provides additional background information for this report.</p>

<h3>Hydra Groups</h3>

<p>The <a href="https://wiki.duraspace.org/display/hydra/Hydra+Metadata+Interest+Group">Hydra Metadata Interest Group</a> has multiple subgroups that have developed best practices for <a href="https://wiki.duraspace.org/display/hydra/Technical+Metadata+Application+Profile">technical metadata</a>, <a href="https://wiki.duraspace.org/display/hydra/Rights+Metadata+Recommendation">rights metadata</a>, <a href="https://wiki.duraspace.org/display/hydra/Rights+Metadata+Recommendation">Segment of a File structural metadata</a>, and <a href="https://wiki.duraspace.org/display/hydra/Applied+Linked+Data+Working+Group">Applied Linked Data</a>. The best practices and metadata application profiles developed by these groups can help in the assessment of metadata quality, but the work of these groups has not yet explicitly included metadata assessment.  The <a href="https://wiki.duraspace.org/display/hydra/Hydra+Metrics+Interest+Group">Hydra Metrics Interest Group</a> is involved in the use of scholarly and web metrics to assess the performance of various aspects of Hydra instances.</p>

<h3>SAA</h3>

<p>Although very little directly related to metadata assessment is available from the SAA, the 2010 presentation by Joyce Celeste Chapman, <a href="http://www2.archivists.org/sites/all/files/saa_description_presentation_2010_chapman.pdf">“User Feedback and Cost/Value Analysis of Metadata Creation”</a> contains many findings that merit consideration. This project studied the information seeking behavior of researchers and regarded successful searches as indicative of the value of metadata. The fields used most often by researchers were identified and the time needed to create metadata for those fields was analyzed in order to determine if the time spent creating metadata was related to the frequency of researcher use.</p>

<h3>ALA ALCTS "Big Heads"</h3>
<p><em>(i.e. “ALA ALCTS Technical Services Directors of Large Research Libraries IG (Big Heads)”)</em></p>

<p>The <a href="http://connect.ala.org/files/7981/costvaluetaskforcereport2010_06_18_pdf_77542.pdf">Final Report of the Task Force on Cost/Value Assessment of Bibliographic Control</a> defines the value of metadata as:</p>

<ol>
  <li>Discovery success</li>
  <li>Use</li>
  <li>Display understanding</li>
  <li>Ability of our data to operate on the open web and interoperate with vendors/ suppliers in the bibliographic supply chain</li>
  <li>Ability to support the FRBR user tasks</li>
  <li>Throughput/Timeliness</li>
  <li>Ability to support the library’s administrative/management goals</li>
</ol>

<p>The use of “ability to support the FRBR user tasks” as a criterion for assessment of metadata quality was cited in Chapman’s (2010) presentation as an indicator of metadata quality.</p>

<p>The report found that describing the cost of metadata is extremely difficult, especially when considering the various operations that support and enable the creation of metadata. The authors acknowledge that the nebulous definitions of value outlined in the report also create challenges for defining what is meant by “cost” in this context.</p>

<h3>USETDA</h3>
<p><em>(i.e. “US Electronic Thesis and Dissertation Association”)</em></p>

<p>The 2015 presentation <a href="http://digital.library.unt.edu/ark:/67531/metadc725793/">“Understanding User Discovery of ETD: Metadata or Full-Text, How Did They Get There?”</a> describes the use of web traffic for metadata analysis. The percentage of successful searches that included terms from an item’s descriptive metadata vs. the percentage of successful searches that included terms from the full text of an item was analyzed to determine how often descriptive metadata contributed to the discovery of an item.</p>

<h3>ALCTS/LITA Metadata Standards</h3>

<p>This joint committee has recently drafted <a href="http://metaware.buzz/2015/10/27/draft-principles-for-evaluating-metadata-standards/">“Principles for Evaluating Metadata Standards”</a>, which provides seven principles intended to apply to the “development, maintenance, governance, selection, use, and assessment of metadata standards” by LAM organizations. The principles recommend metadata standards that meet real-world needs, are open, flexible, and actively maintained, and that support network connections and interoperability. A recent committee blog post <a href="http://metaware.buzz/2016/04/18/summary-of-comments-received-on-msc-principles-for-evaluating-metadata-standards/">summarizes and responds to public comments</a> made on the initial draft, with a subsequent draft expected later this spring.</p>

        </section>
    

    
        <section id="review-pres" class="h2">
            
                    <h2>Presentations</h2>
                

            <p><a href="https://docs.google.com/document/d/1rk6TThrSqpLNk-L0JgR3lk5b_M3M8n5xM2xggKHYVUw/edit#heading=h.mymr4zhj8h6q">Draft and Notes for this Section</a></p>

<h3>Summary</h3>

<p>Being written.</p>

<h3>Themes in Presentations on Metadata Assessment</h3>

<p>Being written.</p>

<h3>Presentations By Year &amp; Conference</h3>

<h4>2003</h4>

<h5>DLF Forum</h5>

<h4>2015</h4>

<h5>DPLAFest</h5>

<h5>ELAG</h5>

<h5>ALA Annual</h5>

<h5>DCMI</h5>

<h5>Tennessee SHAREfest</h5>

<h5>DLF Forum</h5>

<h5>LITA Forum</h5>

<h5>SWIB</h5>

<h4>2016</h4>

<h5>Code4Lib</h5>

<h5>DPLAFest</h5>

        </section>
    

    
        <section id="review-pubs" class="h2">
            
                    <h2>Publications</h2>
                

            <p><a href="https://docs.google.com/document/d/1rk6TThrSqpLNk-L0JgR3lk5b_M3M8n5xM2xggKHYVUw/edit#heading=h.kuaegzwrb5au">Draft and Notes for this Section</a></p>

<h3>Summary</h3>

<p>Being written.</p>

<h3>Publications of Note</h3>

<p>Being written.</p>

<h3>Full Citations List</h3>

<p>We have and will continue to collect citations of interest in <a href="https://www.zotero.org/groups/metadata_assessment">this Zotero Group</a>. We welcome any additions or updates you want to offer to that list. Below is the list of citations contained in that Zotero list, updated on this site periodically:</p>

<p>Being pulled.</p>

        </section>
    

    
        <section id="review-tools" class="h2">
            
                    <h2>Tools</h2>
                

            <p><a href="https://docs.google.com/document/d/1rk6TThrSqpLNk-L0JgR3lk5b_M3M8n5xM2xggKHYVUw/edit#heading=h.nte3qkd91px7">Draft and Notes for this Section</a></p>

<h3>Summary</h3>

<p>Being written.</p>

<h3>GUI Tools for Assessing Metadata</h3>

<h4>NCDHC DPLA Aggregation tools</h4>
<ul>
  <li><a href="https://github.com/ncdhc/dpla-aggregation-tools">North Carolina Digital Heritage Center (NCDHC) DPLA Aggregation Tools link</a></li>
</ul>

<p>This set of tools provides a way of visually browsing metadata from OAI-PMH feeds, with the option to check for values in required fields. Data is displayed in grids, allowing a user to more effectively assess an entire set/collection. Can be particularly useful for people who would like to assess the metadata available over OAI-PMH but who are not comfortable reviewing XML. While the tools are set to review simple dublin core and a set of required fields that applies to NCDHC, this can be modified by changing the code to review a qualified dublin core OAI-PMH feed, and the setting for required fields can also be adjusted. At the University of Utah, we are using these tools (modified by the Mountain West Digital Library) to assess mappings and required field values for legacy collections.</p>

<h4>OpenRefine</h4>
<ul>
  <li><a href="http://openrefine.org/">OpenRefine link</a></li>
  <li><a href="https://github.com/OpenRefine/OpenRefine">Source Code link</a></li>
</ul>

<p>OpenRefine is a free, open source data normalization and reconciliation tool that runs locally in a web browser. Can work with large sets of data, but does best processing &lt;100k lines at a time. Users can utilize faceted search and browsing to identify similar data, or rely on the built-in, super-charged algorithms that suggest ‘clusters’ of data that OpenRefine thinks can be normalized to a single value (including suggesting the ‘best’ value based on relevancies defined in the algorithm). Very useful for assessing and migrating legacy metadata from different systems, and plays well with lots of standard data storage formats (CSV and other delimited files, RDF, XML, JSON, etc). Advanced users can explore OpenRefine as a tool for linking existing data to external sources (eg FreeBase) or normalizing data using programming languages for complex queries. Relatively short learning curve for ‘basic’ level of usage - common actions have built in buttons, pretty intuitive navigation and design, and import/export is very simple.</p>

<p>Openrefine.org provides easy-to-understand video tutorials, in addition to text-based documentation. They also have a documentation wiki <a href="https://github.com/OpenRefine/OpenRefine/wiki/Documentation-For-Users">here</a></p>

<h4>LODrefine</h4>
<ul>
  <li><a href="https://github.com/sparkica/LODRefine">LODRefine link</a></li>
</ul>

<p>LOD-enabled version of OpenRefine. No longer actively supported/maintained.</p>

<h3>GUI Tools for Statistical Computing</h3>

<h4>SPSS</h4>
<ul>
  <li><a href="http://www-01.ibm.com/software/analytics/spss/">SPSS link</a></li>
</ul>

<p>SPSS is a statistical analysis tool widely used in the social sciences, commercially available from IBM. It can be useful for identifying meaningful relationships between variables.</p>

<h3>Scripts for Assessing Metadata</h3>

<h4>Metadata Breakers</h4>
<ul>
  <li><a href="https://github.com/vphill/metadata_breakers">Metadata Breakers</a></li>
</ul>

<p>These Python 2 scripts by Mark Phillips allows you to parse digital library metadata exposed in an OAI-PMH repository.</p>

<h4>Completeness rating in Europeana</h4>
<ul>
  <li><a href="https://docs.google.com/document/d/1Henbc0lQ3gerNoWUd5DcPnNq4YxOxDW5SQ7g4f26Py0/edit#heading=h.l2fg46yn5tej">Completeness rating in Europeana link</a></li>
</ul>

<p>This Java program assigns point-based values to “score” individual metadata records for completeness and assumed “information value” [attractiveness?] to humans. The score is used to increase the visibility of best record in the Europeana portal by boosting their  ranking. Logic for points awarded to a record is laid out in supporting documentation.</p>

<p>Notes from Borys Omelayenko (in-line comment via Github): “It gives rank from 0 to 10 for a record, that consists of two parts: up to 5 points for tags with values (potentially) coming from controlled vocabularies, and up to 5 points for free-text fields.” (line 27-52)</p>

<h3>Other Programming Resources</h3>

<h4>R</h4>
<p>Summary being written.</p>

<h4>R Studio</h4>
<p>Free and open-source integrated development environment (IDE) for R, a programming language for statistical computing and graphics﻿.</p>

<h4>D3</h4>
<p>Tool for data visualization. D3 is a JavaScript library for visualizing data with HTML, SVG, and CSS.</p>

<h4>Plot.ly</h4>
<p>An online analytics and data visualization tool﻿</p>

<h4>Anaconda distribution of Python</h4>
<p>Python is a programming language, and the Anaconda distribution of Python comes bundled with additional things useful for metadata assessment, including: scientific &amp; data analysis libraries, including scikitlearn, pandas, numpy, scipy, nltk, and many more.</p>

<h4>Python pandas</h4>
<p>Python library for analyzing data. See “Anaconda distribution of Python” above</p>

<h3>Data Infrastructure Tools</h3>

<h4>Apache Spark</h4>
<p><a href="http://spark.apache.org/">Apache Spark Link</a></p>

<p>A fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing.﻿ It’s an open source cluster computing framework.</p>

<h4>Hadoop</h4>
<p>Hadoop is an open-source software framework for storing data and running applications on clusters of commodity hardware. It provides massive storage for any kind of data, enormous processing power and the ability to handle virtually limitless concurrent tasks or jobs (summary from <a href="﻿www.sas.com/en_us/insights/big-data/hadoop.html">here</a>).</p>

<h3>Other</h3>

<h4>Google Analytics</h4>
<p>Summary being written.</p>

<h4>Tableau</h4>
<p><a href="http://www.tableau.com/">Link</a></p>

<p>Array of commercially available tools for big data &amp; analytics analysis.</p>

        </section>
    

    
        <section id="citations" class="h2">
            
                    <h2>Citations</h2>
                

            <p>This will contain a list of citations for all the resources, tools, publications, presentations, and other mentioned in the above environmental scan.</p>

        </section>
    

    
        <section id="resources" class="h1">
            
                    <h1>Resources &amp; Contact</h1>
                

            <p><a href="#tbd">Download this resource as a PDF</a></p>

<p>This resource is a work in progress by the <a href="https://wiki.diglib.org/Assessment:Metadata">DLF AIG Metadata Working Group</a>. Get in touch with us via our <a href="https://groups.google.com/forum/#!forum/dlf-aig-metadata-assessment-working-group">Google Group</a>.</p>

        </section>
    


            </div>
        </div>

        <script src="js/jquery-2.1.1.min.js"></script>
        <script src="js/jquery.scrollTo.js"></script>
        <script src="js/jquery.nav.js"></script>
        <script src="js/scripts.js"></script>
        <!-- Google Analytics Tracking Code - replace with you own analytics code -->
        <script>
            (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
            ga('create', 'UA-50358022-3', 'auto');
            ga('send', 'pageview');
        </script>
    </body>
</html>
