---
sectionclass: h2
sectionid: review-orgs
parent-id: review
is-parent: yes
number: 31
title: Organizations &amp; Groups
---
[Draft and Notes for this Section](https://docs.google.com/document/d/1rk6TThrSqpLNk-L0JgR3lk5b_M3M8n5xM2xggKHYVUw/edit#heading=h.ur4v6u3nkorp)

<h3>Summary</h3>

A wide range of groups are addressing issues related to metadata and issues related to assessment of library services, but relatively few are directly working on the assessment of metadata. Here are some organizations and resources of interest.

<h3>Europeana</h3>

[Europeana](http://pro.europeana.eu/) is actively working to develop quality standards for metadata. The [Data Quality Committee](http://pro.europeana.eu/page/data-quality-committee) is addressing many issues related to metadata, including required elements for ingest of EDM data and meaningful metadata values in the context of use. “This work includes measures for information value of statements (informativeness, degree of multilinguality...) “ (p. 3). Of particular note is the committee’s statement on data quality: “The Committee considers that data quality is always relative to intended use and cannot be analysed or defined in isolation from it, as a theoretical effort” (p. 1).

[Europeana’s Report and Recommendations from the Task Force on Metadata Quality](http://pro.europeana.eu/files/Europeana_Professional/Publications/Metadata%20Quality%20Report.pdf) is an essential read, outlining broad issues related to metadata quality as well as specific recommendations for the Europeana community. This report defines good metadata quality as “1. Resulting from a series of trusted processes 2. Findable 3. Readable 4. Standardised 5. Meaningful to audiences 6. Clear on re-use 7. Visible” (p. 3). In addition, the report explores hindrances to good metadata quality: lack of foresight for online discovery, treating metadata as an afterthought, lack of funding and resources, describing digitized items with little information, and not understanding the harvesting requirements.

The [Task Force on Enrichment and Evaluation’s Final Report](http://pro.europeana.eu/files/Europeana_Professional/EuropeanaTech/EuropeanaTech_taskforces/Enrichment_Evaluation/FinalReport_EnrichmentEvaluation_102015.pdf) provides ten recommendations for successful enrichment strategies. See Valentine Charles and Juliane Stiller’s presentation [Evaluation of Metadata Enrichment Practices in Digital Libraries](https://www.youtube.com/watch?v=U90Ajgjk6ic) provides additional background information for this report.

<h3>Hydra Groups</h3>

The [Hydra Metadata Interest Group](https://wiki.duraspace.org/display/hydra/Hydra+Metadata+Interest+Group) has multiple subgroups that have developed best practices for [technical metadata](https://wiki.duraspace.org/display/hydra/Technical+Metadata+Application+Profile), [rights metadata](https://wiki.duraspace.org/display/hydra/Rights+Metadata+Recommendation), [Segment of a File structural metadata](https://wiki.duraspace.org/display/hydra/Rights+Metadata+Recommendation), and [Applied Linked Data](https://wiki.duraspace.org/display/hydra/Applied+Linked+Data+Working+Group). The best practices and metadata application profiles developed by these groups can help in the assessment of metadata quality, but the work of these groups has not yet explicitly included metadata assessment.  The [Hydra Metrics Interest Group](https://wiki.duraspace.org/display/hydra/Hydra+Metrics+Interest+Group) is involved in the use of scholarly and web metrics to assess the performance of various aspects of Hydra instances.

<h3>SAA</h3>

Although very little directly related to metadata assessment is available from the SAA, the 2010 presentation by Joyce Celeste Chapman, [“User Feedback and Cost/Value Analysis of Metadata Creation”](http://www2.archivists.org/sites/all/files/saa_description_presentation_2010_chapman.pdf) contains many findings that merit consideration. This project studied the information seeking behavior of researchers and regarded successful searches as indicative of the value of metadata. The fields used most often by researchers were identified and the time needed to create metadata for those fields was analyzed in order to determine if the time spent creating metadata was related to the frequency of researcher use.

<h3>ALA ALCTS "Big Heads"</h3>
*(i.e. "ALA ALCTS Technical Services Directors of Large Research Libraries IG (Big Heads)")*

The [Final Report of the Task Force on Cost/Value Assessment of Bibliographic Control](http://connect.ala.org/files/7981/costvaluetaskforcereport2010_06_18_pdf_77542.pdf) defines the value of metadata as:

1. Discovery success
2. Use
3. Display understanding
4. Ability of our data to operate on the open web and interoperate with vendors/ suppliers in the bibliographic supply chain
5. Ability to support the FRBR user tasks
6. Throughput/Timeliness
7. Ability to support the library's administrative/management goals

The use of “ability to support the FRBR user tasks” as a criterion for assessment of metadata quality was cited in Chapman’s (2010) presentation as an indicator of metadata quality.

The report found that describing the cost of metadata is extremely difficult, especially when considering the various operations that support and enable the creation of metadata. The authors acknowledge that the nebulous definitions of value outlined in the report also create challenges for defining what is meant by “cost” in this context.

<h3>USETDA</h3>
*(i.e. "US Electronic Thesis and Dissertation Association")*

The 2015 presentation [“Understanding User Discovery of ETD: Metadata or Full-Text, How Did They Get There?”](http://digital.library.unt.edu/ark:/67531/metadc725793/) describes the use of web traffic for metadata analysis. The percentage of successful searches that included terms from an item’s descriptive metadata vs. the percentage of successful searches that included terms from the full text of an item was analyzed to determine how often descriptive metadata contributed to the discovery of an item.

<h3>ALCTS/LITA Metadata Standards</h3>

This joint committee has recently drafted ["Principles for Evaluating Metadata Standards"](http://metaware.buzz/2015/10/27/draft-principles-for-evaluating-metadata-standards/), which provides seven principles intended to apply to the "development, maintenance, governance, selection, use, and assessment of metadata standards" by LAM organizations. The principles recommend metadata standards that meet real-world needs, are open, flexible, and actively maintained, and that support network connections and interoperability. A recent committee blog post [summarizes and responds to public comments](http://metaware.buzz/2016/04/18/summary-of-comments-received-on-msc-principles-for-evaluating-metadata-standards/) made on the initial draft, with a subsequent draft expected later this spring.
